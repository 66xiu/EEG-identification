# 非线性激活
## RELU
https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU

```
import torch
from torch import nn
from torch.nn import ReLU

input = torch.tensor([[1, -0.5],
                      [-1, 3]])
input = torch.reshape(input, (-1, 1, 2, 2))
print(input.shape)


class module(nn.Module):
    def __init__(self):
        super(module, self).__init__()
        self.relu1 = ReLU()

    def forward(self, input):
        output = self.relu1(input)
        return output


m = module()
output = m(input)
print(output)
```

## sigmoid
https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid
```
import torch
from torch import nn
from torch.nn import Sigmoid

input = torch.tensor([[1, -0.5],
                      [-1, 3]])
input = torch.reshape(input, (-1, 1, 2, 2))
print(input.shape)


class module(nn.Module):
    def __init__(self):
        super(module, self).__init__()
        self.sigmoid1 = Sigmoid()

    def forward(self, input):
        output = self.sigmoid1(input)
        return output


m = module()
output = m(input)
print(output)

```
